{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "***\n",
    "*Course:* [Math 535](http://www.math.wisc.edu/~roch/mmids/) - Mathematical Methods in Data Science (MMiDS) - Fall 2020\n",
    "\n",
    "*Name:* EDIT: write your name here \n",
    "***\n",
    "\n",
    "# <span style=\"background-color:purple; color:white; padding:2px 6px\">HWK 9</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "\n",
    "1. Install Julia and Jupyter notebooks by following [these instructions](https://datatofish.com/add-julia-to-jupyter/).\n",
    "\n",
    "2. Add the necessary packages by following [these instructions](https://datatofish.com/install-package-julia/).\n",
    "\n",
    "3. Download the required dataset from Canvas.\n",
    "\n",
    "4. In the Jupyter notebook, write your name above and do the exercises below.\n",
    "\n",
    "5. Run every cell.\n",
    "\n",
    "6. \"Download as\" HTML. Before this step, make sure to \"Restart & Run All\" in the \"Kernel\" menu. The cell outputs should be numbered in order. \n",
    "\n",
    "7. Open the HTML file in a browser and save it as a PDF.\n",
    "\n",
    "8. Due date and instructions to submit your PDF will be given on Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mmids_backsubs(U,b):\n",
    "    m = b.shape[0]\n",
    "    x = np.zeros(m)\n",
    "    for i in reversed(range(m)):\n",
    "        x[i] = (b[i] - np.dot(U[i,i+1:m],x[i+1:m]))/U[i,i]\n",
    "    return x\n",
    "\n",
    "def mmids_forwardsubs(L,b):\n",
    "    m = b.shape[0]\n",
    "    x = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        x[i] = (b[i] - np.dot(L[i,0:i],x[0:i]))/L[i,i]\n",
    "    return x\n",
    "\n",
    "def mmids_cholesky(B):\n",
    "    n = B.shape[0] # number of rows\n",
    "    L = np.zeros((n, n)) # initiallization of L\n",
    "    for j in range(n):\n",
    "        L[j,0:j] = mmids_forwardsubs(L[0:j,0:j],B[j,0:j])\n",
    "        L[j,j] = np.sqrt(B[j,j] - LA.norm(L[j,0:j])**2)\n",
    "    return L \n",
    "\n",
    "\n",
    "def wls_by_chol(A, y, w):\n",
    "    W = np.diag(w)\n",
    "    C = A.T @ W @ A #Computes C (Step I)\n",
    "    M = mmids_cholesky(C) #Uses mmids_cholesky to compute M (Step II)\n",
    "    z = mmids_forwardsubs(M, A.T @ W @ y) #Uses mmids_forwardsubs to compute z (Step III)\n",
    "    return mmids_backsubs(M.T, z) #Uses mmids_backsubs to return coefficients (Step IV)\n",
    "\n",
    "def sigmoid(z): \n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF RUNNING ON GOOGLE COLAB\n",
    "# When prompted, upload: \n",
    "#     * lebron.csv\n",
    "#     * SAHeart.csv\n",
    "# from your local file system\n",
    "# Data source: https://www.math.wisc.edu/~roch/mmids/\n",
    "# Alternative instructions: https://colab.research.google.com/notebooks/io.ipynb\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background** The goal of this assignment is to implement a solution algorithm for logistic regression known as Iterative Reweighted Least Squares (IRLS) which may be preferable to gradient descent in this context, [at least for datasets of moderate dimension](https://www.youtube.com/watch?v=iwO0JPt59YQ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first recall some basic facts about logistic regression. Our input data is of the form $\\{(\\boldsymbol{\\alpha}_i, b_i) : i=1,\\ldots, n\\}$ where $\\boldsymbol{\\alpha}_i \\in \\mathbb{R}^d$ are the features and $b_i \\in \\{0,1\\}$ is the label. As before we use a matrix representation: $A \\in \\mathbb{R}^{n \\times d}$ has rows $\\boldsymbol{\\alpha}_j^T$, $j = 1,\\ldots, n$ and $\\mathbf{b} = (b_1, \\ldots, b_n)^T \\in \\{0,1\\}^n$. Our goal is to find a function of the features that approximates the probability of the label $1$. For this purpose, we model the [log-odds](https://en.wikipedia.org/wiki/Logit) (or logit function) of the probability of label $1$ as a linear function of the features\n",
    "\n",
    "$$\n",
    "\\log \\frac{p(\\boldsymbol{\\alpha}; \\mathbf{x})}{1-p(\\boldsymbol{\\alpha}; \\mathbf{x})}\n",
    "= \\boldsymbol{\\alpha}^T \\mathbf{x}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x} \\in \\mathbb{R}^d$. Inverting this expression gives\n",
    "$\n",
    "p(\\boldsymbol{\\alpha}; \\mathbf{x})\n",
    "= \\sigma(\\boldsymbol{\\alpha}^T \\mathbf{x})\n",
    "$\n",
    "where the [sigmoid](https://en.wikipedia.org/wiki/Logistic_function) function is\n",
    "\n",
    "$$\n",
    "\\sigma(t)\n",
    "= \\frac{1}{1 + e^{-t}}\n",
    "$$\n",
    "\n",
    "for $t \\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to minimize the [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression) over $\\mathbf{x} \\in \\mathbb{R}^d$\n",
    "\n",
    "$$\n",
    "\\ell(\\mathbf{x}; A, \\mathbf{b})\n",
    "= - \\sum_{i=1}^n b_i \\log(\\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x}))\n",
    "- \\sum_{i=1}^n (1-b_i) \\log(1- \\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x})).\n",
    "$$\n",
    "\n",
    "We showed previously that the function $\\ell(\\mathbf{x}; A, \\mathbf{b})$ is convex as a function of $\\mathbf{x} \\in \\mathbb{R}^d$. Its gradient with respect to the parameters $\\mathbf{x}$ is \n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{x}}\\,\\ell(\\mathbf{x}; A, \\mathbf{b})\n",
    "=  \\sum_{i=1}^n (\n",
    "\\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x}) \n",
    "-\n",
    "b_i\n",
    ") \\,\\boldsymbol{\\alpha}_i\n",
    "$$\n",
    "\n",
    "and its Hessian is\n",
    "\n",
    "$$\n",
    "\\nabla^2_{\\mathbf{x}} \\,\\ell(\\mathbf{x}; A, \\mathbf{b})\n",
    "=  \\sum_{i=1}^n \\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x}) (1 - \\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x}))\\, \\boldsymbol{\\alpha}_i \\boldsymbol{\\alpha}_i^T\n",
    "$$\n",
    "\n",
    "where $\\nabla^2_{\\mathbf{x}}$ indicates the Hessian with respect to the $\\mathbf{x}$ variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For step size $\\beta$, one step of gradient descent is therefore\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{k+1}\n",
    "= \\mathbf{x}^{k} - \\beta \\sum_{i=1}^n (\n",
    "\\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x}^k) - b_i\n",
    ") \\,\\boldsymbol{\\alpha}_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A: Implementing IRLS** The IRLS algorithm is based on [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) which takes advantage of second-order, i.e., Hessian information to select an update direction. We only give heuristic derivation here. Let $\\mathbf{x}^k$ be the current iterate and consider the quadratic approximation of $\\ell(\\mathbf{x}; A, \\mathbf{b})$ around $\\mathbf{x}^k$ obtained from the *Multivariate Taylor* expansion\n",
    "\n",
    "$$\n",
    "\\ell(\\mathbf{x}; A, \\mathbf{b})\n",
    "\\approx\n",
    "\\ell(\\mathbf{x}^k; A, \\mathbf{b})\n",
    "+ \\nabla_{\\mathbf{x}}\\,\\ell(\\mathbf{x}; A, \\mathbf{b})^T (\\mathbf{x} - \\mathbf{x}^k)\n",
    "+ \\frac{1}{2} (\\mathbf{x} - \\mathbf{x}^k)^T \\nabla^2_{\\mathbf{x}} \\,\\ell(\\mathbf{x}^k; A, \\mathbf{b}) (\\mathbf{x} - \\mathbf{x}^k).\n",
    "$$\n",
    "\n",
    "To choose the new iterate, we minimize this quadratic approximation. We have previously shown that the minimum is achieved at \n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{k+1} - \\mathbf{x}^k\n",
    "= - \\nabla^2_{\\mathbf{x}} \\,\\ell(\\mathbf{x}^k; A, \\mathbf{b})^{-1} \\nabla_{\\mathbf{x}}\\,\\ell(\\mathbf{x}; A, \\mathbf{b})\n",
    "$$\n",
    "\n",
    "provided the inverse is well-defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To derive a more explicit expression, we first simplify the gradient and Hessian above by introducing the notation $\\mathbf{z}^k = (z_1^k,\\ldots,z_n^k)$ with $z_i^k = \\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x}^k)$\n",
    "and $W_k = \\mathrm{diag}(z_1^k (1-z_1^k),\\ldots,z_n^k (1-z_n^k))$. Then we get\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{x}}\\,\\ell(\\mathbf{x}^k; A, \\mathbf{b})\n",
    "=  \\sum_{i=1}^n (\n",
    "\\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x}^k) \n",
    "-\n",
    "b_i\n",
    ") \\,\\boldsymbol{\\alpha}_i\n",
    "= \\sum_{i=1}^n (\n",
    "z_i^k\n",
    "-\n",
    "b_i\n",
    ") \\,\\boldsymbol{\\alpha}_i\n",
    "= A^T (\\mathbf{z}^k - \\mathbf{b})\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\nabla^2_{\\mathbf{x}} \\,\\ell(\\mathbf{x}^k; A, \\mathbf{b})\n",
    "=  \\sum_{i=1}^n \\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x}^k) (1 - \\sigma(\\boldsymbol{\\alpha}_i^T \\mathbf{x}^k))\\, \\boldsymbol{\\alpha}_i \\boldsymbol{\\alpha}_i^T\n",
    "=  \\sum_{i=1}^n z_i^k (1 - z_i^k)\\, \\boldsymbol{\\alpha}_i \\boldsymbol{\\alpha}_i^T\n",
    "= A^T W_k A.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging these back into the formula for the next iterate, we arrive at\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{k+1}\n",
    "= \\mathbf{x}^k + (A^T W_k A)^{-1} A^T (\\mathbf{b} - \\mathbf{z}^k).\n",
    "$$\n",
    "\n",
    "We rearrange this last expression to highlight some interesting structure. \n",
    "Note that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{x}^{k+1}\n",
    "&= (A^T W_k A)^{-1}(A^T W_k A \\mathbf{x}^k + A^T\\mathbf{b} - A^T\\mathbf{z}^k)\\\\\n",
    "&= (A^T W_k A)^{-1} A^T W_k( A \\mathbf{x}^k + W_k^{-1} (\\mathbf{b} - \\mathbf{z}^k))\\\\\n",
    "&= (A^T W_k A)^{-1} A^T W_k \\mathbf{y}^k\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where we defined the working response $\\mathbf{y}^k = A \\mathbf{x}^k + W_k^{-1} (\\mathbf{b} - \\mathbf{z}^k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may recognize from a previous assignment that $\\mathbf{x}^{k+1}$ is the solution to the weighted normal equations\n",
    "\n",
    "$$\n",
    "A^T W_k A \\mathbf{x}^{k+1}\n",
    "= A^T W_k \\mathbf{y}^k.\n",
    "$$\n",
    "\n",
    "That connnection explains the name Iterative Reweighted Least Squares. You will use it below to implement the method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Complete the function below which updates the weights $w_i^k = z^k_i (1 - z_i^k)$ and the responses $y^k_i$. Use the fact that the components of $\\mathbf{y}^k$ can be simplified to\n",
    "\n",
    "$$\n",
    "y_i^k \n",
    "= (A \\mathbf{x}^k)_i + \\frac{b_i - z_i^k}{w_i^k}.\n",
    "$$\n",
    "\n",
    "Furthermore, make use of the `sigmoid` function defined in the first code cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_w_and_y(x, A, b):\n",
    "    z = #EDIT: Compute z\n",
    "    w = #EDIT: Compute w\n",
    "    y = #EDIT: Compute y \n",
    "    return w, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Complete the main routine of the IRLS algorithm. Make use of the `wls_by_chol` function in the first code cell above, which you will recall from a previous assignment solves the weighted least squares problem. And of course make use of `update_w_and_y`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmids_irls(A, b, maxiter=int(1e2)):\n",
    "    \n",
    "    (n,m) = A.shape\n",
    "    xk = #EDIT: Initialize x to the zero vector\n",
    "    \n",
    "    for _ in range(maxiter):\n",
    "        wk,yk = #EDIT: Update w and y        \n",
    "        xk = #EDIT: Compute next x iterate\n",
    "    \n",
    "    return xk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B: Revisiting NBA dataset** In this part, you will re-analyze the `lebron.csv` dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Load the `lebron.csv` dataset. Extract the feature `shot_distance` and the label `shot_made`. Construct the matrix $A$ and the vector $b$ as defined above. Make sure to include a column of $1$'s in A, as we have done previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDIT: Load `lebron.csv` and construct A and b as instructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** Use `mmids_irls` to solve the logistic regression problem. Make sure to output your estimated $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat = #EDIT: Use IRLS to solve the logistic regression problem on A and b\n",
    "print(x_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C: Revisiting the South African Heart Disease dataset** In this part, you will re-analyze the `SAHeart.csv` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** Load the `SAHeart.csv` dataset. Extract the features `sbp`, `tobacco`, `ldl`, `adiposity`, `typea`, `obesity`, `alcohol`, `age` and the label `chd`. Construct the matrix $A$ and the vector $b$ as defined above. Make sure to include a column of $1$'s in A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDIT: Load `SAHeart.csv` and construct A and b as instructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** Use `mmids_irls` to solve the logistic regression problem. Make sure to output your estimated $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat = #EDIT: Use IRLS to solve the logistic regression problem on A and b\n",
    "print(x_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7:** To get a sense of how accurate the result is, we compare our predictions to the true labels. By prediction, let us say that we mean that we predict label $1$ whenever $\\sigma(\\boldsymbol{\\alpha}^T \\mathbf{x}) > 1/2$. Compute the fraction of correct predictions on the training set using your estimated `x_hat`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDIT: Compute the fraction of correct predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
